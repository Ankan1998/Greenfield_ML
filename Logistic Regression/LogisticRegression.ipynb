{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        # Learning rate(alpha) is a hyper-parameter to determine the value of update of weights and bias on gradient descent\n",
    "        # Too small Learning rate could make convergence too slow\n",
    "        # Too big learning rate cause the descent to jumps the minima\n",
    "        self.learning_rate=learning_rate\n",
    "        # Epochs is the number to time the descent will take place\n",
    "        self.epochs=epochs\n",
    "        # Initialising weights and bias to None\n",
    "        # Weights and bias is tweaked by gradient to find the line of best fit.\n",
    "        # Weight is defined as how much of what features combined to give the Dependent Variable\n",
    "        self.weights=None\n",
    "        self.bias=None\n",
    "        \n",
    "    def predicted_val(Xi,W,b):\n",
    "        # Xi=(num_of_samples,num_of_features);W=(num_of_features,1);b=(num_of_samples,1)\n",
    "        # Dimension of ypred --> (num_of_samples,1)\n",
    "        linearPred=np.dot(Xi,W) + b\n",
    "        ypred=1/(1+np.exp(-linearPred)) # sigmoid function for logistic regression\n",
    "        return ypred\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_of_samples,num_of_features=X.shape\n",
    "        # weight vector is of dimension (num_of_features,1)\n",
    "        self.weights= np.random.randn(num_of_features,1)\n",
    "        # Bias is (1,1)\n",
    "        self.bias=np.random.randn(1,1)\n",
    "        # Looping over number of epochs\n",
    "        for i in range(self.epochs):\n",
    "            # dW is the derivative of the cost function \n",
    "            # Cost function is the mean-squared error between predicted value of y and real y\n",
    "            dW=(X.T@(predicted_val(X,self.weights,self.bias)-y))/(2*len(y))\n",
    "            db=np.sum(predicted_val(X,self.weights,self.bias)-y)/(2*len(y))\n",
    "            \n",
    "            # Updating weights and bias\n",
    "            self.weights=self.weights-self.learning_rate*dW\n",
    "            self.bias=self.bias-self.learning_rate*db\n",
    "            \n",
    "    def predict(self,X):\n",
    "        lin_pred_y=(X@self.weights) +self.bias\n",
    "        y_pred_sig=1/(1+np.exp(-lin_pred_y))\n",
    "        # if any value greater than 0.5 in y_pred_sig then 1 else 0 \n",
    "        y_predicted=[1 if i>0.5 else 0 for i in y_pred_sig]\n",
    "        return y_predicted\n",
    "    \n",
    "    # Checking accuracy\n",
    "    def accuracy(y,ypred):\n",
    "        return np.mean(y==ypred)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
