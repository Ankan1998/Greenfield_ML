{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    \n",
    "    def __init__(self,learning_rate=0.1,epochs=500):\n",
    "        # Learning rate(alpha) is a hyper-parameter to determine the value of update of weights and bias on gradient descent\n",
    "        # Too small Learning rate could make convergence too slow\n",
    "        # Too big learning rate cause the descent to jumps the minima\n",
    "        self.learning_rate=learning_rate\n",
    "        # Epochs is the number to time the descent will take place\n",
    "        self.epochs=epochs\n",
    "        # Initialising weights and bias to None\n",
    "        # Weights and bias is tweaked by gradient to find the line of best fit.\n",
    "        # Weight is defined as how much of what features combined to give the Dependent Variable\n",
    "        self.weights=None\n",
    "        self.bias=None\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        # sigmoid function for logistic regression\n",
    "        return 1/(1+np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def predicted_val(self,Xi,W,b):\n",
    "        # Xi=(num_of_samples,num_of_features);W=(num_of_features,1);b=(num_of_samples,1)\n",
    "        # Dimension of ypred --> (num_of_samples,1)\n",
    "        linearPred=np.dot(Xi,W) + b\n",
    "        # Passing the prediction through sigmoid\n",
    "        # To simply explain, is to bring down value between 0 and 1\n",
    "        ypred=self.sigmoid(linearPred) \n",
    "        return ypred\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_of_samples,num_of_features=X.shape\n",
    "        # reshape y because if y is a rank matrix -->(num_of_samples,)\n",
    "        # Then there will be a issues in dimension\n",
    "        # For better understanding. visit this link: https://www.kaggle.com/getting-started/176510\n",
    "        y=y.reshape(num_of_samples,1)\n",
    "        # weight vector is of dimension (num_of_features,1)\n",
    "        self.weights= np.random.randn(num_of_features,1)\n",
    "        # Bias is (1,1)\n",
    "        self.bias=np.random.randn(1,1)\n",
    "        # Looping over number of epochs\n",
    "        for i in range(self.epochs):\n",
    "            # dW is the derivative of the cost function \n",
    "            # Cost function is the binary or sigmoid cross-entropy −(ylog(p)+(1−y)log(1−p))\n",
    "            # p=predicted\n",
    "            dW=np.dot(X.T,(self.predicted_val(X,self.weights,self.bias)-y))/(num_of_samples)\n",
    "            db=np.sum(self.predicted_val(X,self.weights,self.bias)-y)/(num_of_samples)\n",
    "            \n",
    "            # Updating weights and bias\n",
    "            self.weights=self.weights-self.learning_rate*dW\n",
    "            self.bias=self.bias-self.learning_rate*db\n",
    "            \n",
    "    def predict(self,X):\n",
    "        lin_pred_y=np.dot(X,self.weights) + self.bias\n",
    "        y_pred_sig=self.sigmoid(lin_pred_y) \n",
    "        # if any value greater than 0.5 in y_pred_sig then 1 else 0 \n",
    "        y_predicted = [1 if i > 0.5 else 0 for i in y_pred_sig]\n",
    "        return np.array(y_predicted)\n",
    "    \n",
    "    # Checking accuracy\n",
    "    def accuracy(self,y,ypredi):\n",
    "        return np.mean(y==ypredi)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data\n",
    "from sklearn import datasets\n",
    "X,y=datasets.make_classification(n_samples=1000,n_features=6,n_classes=2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting test,train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=Logistic_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.accuracy(y_test,p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
